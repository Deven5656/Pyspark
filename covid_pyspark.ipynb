{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"<br>\n",
    "    @Author: Deven Gupta<br>\n",
    "    @Date: 3-09-2024<br>\n",
    "    @Last Modified by: Deven Gupta<br>\n",
    "    @Last Modified time: 3-09-2024<br>\n",
    "    @Title : Covid dataset Problem using pyspark sql<br>\n",
    "<br>\n",
    "\"\"\"<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('SQL_Pyspark').master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-GS37EC8:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SQL_Pyspark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22ccaf0cbc0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing/Reading CSV Files\n",
    "\n",
    "df_country_wise_latest = spark.read.csv(\"file:///D:/BridgeLabz DE/covid-kaggle-dataset/country_wise_latest.csv\",header=True,inferSchema=True)\n",
    "df_worldometer_data = spark.read.csv(\"file:///D:/BridgeLabz DE/covid-kaggle-dataset/worldometer_data.csv\",header=True,inferSchema=True)\n",
    "df_covid_19_clean_complete = spark.read.csv(\"file:///D:/BridgeLabz DE/covid-kaggle-dataset/covid_19_clean_complete.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Temporary View\n",
    "\n",
    "df_country_wise_latest.createOrReplaceTempView(\"country_wise_latest\")\n",
    "df_worldometer_data.createOrReplaceTempView(\"worldometer_data\")\n",
    "df_covid_19_clean_complete.createOrReplaceTempView(\"covid_19_clean_complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Table\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|Country/Region|Confirmed|Deaths|Recovered|Active|New cases|New deaths|New recovered|Deaths / 100 Cases|Recovered / 100 Cases|Deaths / 100 Recovered|Confirmed last week|1 week change|1 week % increase|          WHO Region|\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "|   Afghanistan|    36263|  1269|    25198|  9796|      106|        10|           18|               3.5|                69.49|                  5.04|              35526|          737|             2.07|Eastern Mediterra...|\n",
      "+--------------+---------+------+---------+------+---------+----------+-------------+------------------+---------------------+----------------------+-------------------+-------------+-----------------+--------------------+\n",
      "\n",
      "\n",
      "\n",
      "2nd Table \n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "|Country/Region|    Continent|Population|TotalCases|NewCases|TotalDeaths|NewDeaths|TotalRecovered|NewRecovered|ActiveCases|Serious,Critical|Tot Cases/1M pop|Deaths/1M pop|TotalTests|Tests/1M pop|WHO Region|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "|           USA|North America| 331198130|   5032179|    NULL|     162804|     NULL|       2576668|        NULL|    2292707|           18296|           15194|        492.0|  63139605|      190640|  Americas|\n",
      "+--------------+-------------+----------+----------+--------+-----------+---------+--------------+------------+-----------+----------------+----------------+-------------+----------+------------+----------+\n",
      "\n",
      "\n",
      "\n",
      "3rd Table\n",
      "+--------------+--------------+--------+---------+----------+---------+------+---------+------+--------------------+\n",
      "|Province/State|Country/Region|     Lat|     Long|      Date|Confirmed|Deaths|Recovered|Active|          WHO Region|\n",
      "+--------------+--------------+--------+---------+----------+---------+------+---------+------+--------------------+\n",
      "|          NULL|   Afghanistan|33.93911|67.709953|2020-01-22|        0|     0|        0|     0|Eastern Mediterra...|\n",
      "+--------------+--------------+--------+---------+----------+---------+------+---------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Display Tables\n",
    "\n",
    "table1 = spark.sql(\"\"\"\n",
    "    SELECT * FROM country_wise_latest LIMIT 1;\n",
    "\"\"\")\n",
    "table2 = spark.sql(\"\"\"\n",
    "    SELECT * FROM worldometer_data LIMIT 1;\n",
    "\"\"\")\n",
    "table3 = spark.sql(\"\"\"\n",
    "    SELECT * FROM covid_19_clean_complete LIMIT 1;\n",
    "\"\"\")\n",
    "print(\"1st Table\")\n",
    "table1.show()\n",
    "print(\"\\n\\n2nd Table \")\n",
    "table2.show()\n",
    "print(\"\\n\\n3rd Table\")\n",
    "table3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. To find out the death percentage locally and globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------------+\n",
      "|Country|local_death_percentage|\n",
      "+-------+----------------------+\n",
      "|  India|      2.25718596312479|\n",
      "+-------+----------------------+\n",
      "\n",
      "+-----------------------+\n",
      "|global_death_percentage|\n",
      "+-----------------------+\n",
      "|       3.96854825570971|\n",
      "+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Perform SQL queries using Spark SQL\n",
    "\n",
    "local_death_percentage = spark.sql(\"\"\"                                \n",
    "    SELECT \n",
    "    `Country/Region` AS Country,\n",
    "    (SUM(cast(Deaths as int)) * 100.0 / SUM(cast(Confirmed as int))) AS local_death_percentage\n",
    "    FROM country_wise_latest\n",
    "    GROUP BY `Country/Region` \n",
    "    HAVING `Country/Region`='India';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "global_death_percentage = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    (SUM(cast(Deaths as int)) * 100.0 / SUM(cast(Confirmed as int))) AS global_death_percentage\n",
    "    FROM country_wise_latest;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "local_death_percentage.show()\n",
    "global_death_percentage.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. To find out the infected population percentage locally and globally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------+\n",
      "|Country|Local_Infected_Population_Percentage|\n",
      "+-------+------------------------------------+\n",
      "|  India|                    0.14662586134519|\n",
      "+-------+------------------------------------+\n",
      "\n",
      "+----------+----------------+------------------------------------+\n",
      "|Total_Case|Total_Population|Gobal_Infected_Population_Percentage|\n",
      "+----------+----------------+------------------------------------+\n",
      "|  19169166|      6326421290|                    0.30300173069884|\n",
      "+----------+----------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Local_Infected_Population_Percentage = spark.sql(\"\"\"                                \n",
    "    SELECT \n",
    "    `Country/Region` AS Country,\n",
    "    (SUM(TotalCases) * 100.0 / SUM(Population)) AS Local_Infected_Population_Percentage\n",
    "    FROM worldometer_data\n",
    "    GROUP BY `Country/Region`\n",
    "    HAVING `Country/Region`='India';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Gobal_Infected_Population_Percentage = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    SUM(TotalCases) AS Total_Case,\n",
    "    SUM(cast(Population AS BIGINT)) AS Total_Population,\n",
    "    (SUM(TotalCases) * 100.0 / SUM(cast(Population AS BIGINT))) AS Gobal_Infected_Population_Percentage\n",
    "    FROM worldometer_data;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Local_Infected_Population_Percentage.show()\n",
    "Gobal_Infected_Population_Percentage.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. To find out the countries with the highest infection rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------+\n",
      "|      Country|Highest_Infection_Rate|\n",
      "+-------------+----------------------+\n",
      "|        Qatar|      3.99215757504528|\n",
      "|French Guiana|      2.71456485795882|\n",
      "|      Bahrain|      2.51302390797513|\n",
      "|   San Marino|      2.05963816371030|\n",
      "|        Chile|      1.91648102282847|\n",
      "|       Panama|      1.65270398923282|\n",
      "|       Kuwait|      1.63784431675388|\n",
      "|         Oman|      1.57690439637343|\n",
      "|          USA|      1.51938629605185|\n",
      "| Vatican City|      1.49812734082397|\n",
      "|         Peru|      1.37934516564369|\n",
      "|       Brazil|      1.37161041251279|\n",
      "|      Armenia|      1.34350672158245|\n",
      "|      Andorra|      1.22156370506483|\n",
      "|   Luxembourg|      1.12815654148962|\n",
      "|      Mayotte|      1.11257813100041|\n",
      "|    Singapore|      0.93177854157828|\n",
      "| South Africa|      0.90631493281939|\n",
      "|       Israel|      0.86499833108456|\n",
      "|     Maldives|      0.86434893101461|\n",
      "+-------------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Highest_Infection_Rate = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    `Country/Region` AS Country,\n",
    "    (SUM(TotalCases) * 100.0 / SUM(Population)) AS Highest_Infection_Rate\n",
    "    FROM worldometer_data\n",
    "    GROUP BY `Country/Region` \n",
    "    ORDER BY Highest_Infection_Rate DESC;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Highest_Infection_Rate.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. To find out the countries and continents with the highest death counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|Country|TotalDeaths|\n",
      "+-------+-----------+\n",
      "|    USA|     162804|\n",
      "| Brazil|      98644|\n",
      "| Mexico|      50517|\n",
      "|     UK|      46413|\n",
      "|  India|      41638|\n",
      "+-------+-----------+\n",
      "\n",
      "+-------------+----------------------+\n",
      "|    Continent|Total_Continent_Deaths|\n",
      "+-------------+----------------------+\n",
      "|North America|                229855|\n",
      "|       Europe|                205232|\n",
      "|South America|                154885|\n",
      "|         Asia|                100627|\n",
      "|       Africa|                 22114|\n",
      "+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Country_Highest_Death = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    `Country/Region` AS Country,\n",
    "    TotalDeaths\n",
    "    FROM worldometer_data\n",
    "    ORDER BY TotalDeaths DESC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Total_Continent_Deaths = spark.sql(\"\"\"\n",
    "   SELECT\n",
    "    Continent,\n",
    "    SUM(TotalDeaths) AS Total_Continent_Deaths\n",
    "    FROM worldometer_data\n",
    "    GROUP BY Continent\n",
    "    ORDER BY Total_Continent_Deaths DESC\n",
    "    LIMIT 5;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Country_Highest_Death.show()\n",
    "Total_Continent_Deaths.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Average number of deaths by day (Continents and Countries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------+\n",
      "|      Country/Region|      Date|DEATHS|\n",
      "+--------------------+----------+------+\n",
      "|                Iraq|2020-01-23|   0.0|\n",
      "|             Comoros|2020-01-27|   0.0|\n",
      "|               Niger|2020-01-31|   0.0|\n",
      "|    Papua New Guinea|2020-01-31|   0.0|\n",
      "|  Dominican Republic|2020-02-02|   0.0|\n",
      "|             Albania|2020-02-03|   0.0|\n",
      "|Central African R...|2020-02-03|   0.0|\n",
      "|               Ghana|2020-02-03|   0.0|\n",
      "|              Malawi|2020-02-04|   0.0|\n",
      "|         Switzerland|2020-02-05|   0.0|\n",
      "|          Cabo Verde|2020-02-06|   0.0|\n",
      "|           Lithuania|2020-02-06|   0.0|\n",
      "|             Comoros|2020-02-09|   0.0|\n",
      "|             Czechia|2020-02-10|   0.0|\n",
      "|             Liberia|2020-02-11|   0.0|\n",
      "|          Azerbaijan|2020-02-12|   0.0|\n",
      "|          Madagascar|2020-02-13|   0.0|\n",
      "|         Timor-Leste|2020-02-13|   0.0|\n",
      "|               Egypt|2020-02-14|   0.0|\n",
      "|        Sierra Leone|2020-02-14|   0.0|\n",
      "+--------------------+----------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+--------------------+----------+------------------+\n",
      "|           Continent|      Date|            DEATHS|\n",
      "+--------------------+----------+------------------+\n",
      "|              Africa|2020-03-01|               0.0|\n",
      "|     South-East Asia|2020-03-01|               0.1|\n",
      "|            Americas|2020-04-21|1163.4565217391305|\n",
      "|     South-East Asia|2020-05-16|             435.0|\n",
      "|            Americas|2020-05-24| 3159.217391304348|\n",
      "|              Europe|2020-05-27|         2200.1625|\n",
      "|     South-East Asia|2020-06-01|             800.9|\n",
      "|Eastern Mediterra...|2020-06-07| 663.0454545454545|\n",
      "|            Americas|2020-06-14| 4439.673913043478|\n",
      "|              Africa|2020-01-25|               0.0|\n",
      "|              Europe|2020-02-26|             0.175|\n",
      "|     Western Pacific|2020-05-13|120.98181818181818|\n",
      "|     South-East Asia|2020-05-13|             392.3|\n",
      "|              Europe|2020-05-17|          2092.625|\n",
      "|     Western Pacific|2020-03-16| 60.69090909090909|\n",
      "|     Western Pacific|2020-05-06|116.50909090909092|\n",
      "|Eastern Mediterra...|2020-05-18|460.22727272727275|\n",
      "|Eastern Mediterra...|2020-02-12|               0.0|\n",
      "|     Western Pacific|2020-05-29|127.41818181818182|\n",
      "|            Americas|2020-06-16| 4553.021739130435|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Country_Death = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    `Country/Region`,Date,\n",
    "\tAVG(Deaths) AS DEATHS\n",
    "    FROM covid_19_clean_complete\n",
    "    GROUP BY `Country/Region`,Date;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Continent_Death = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "    `WHO Region` AS Continent,Date,\n",
    "\tAVG(Deaths) AS DEATHS\n",
    "    FROM covid_19_clean_complete\n",
    "    GROUP BY `WHO Region`,Date;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "Country_Death.show()\n",
    "Continent_Death.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Average of cases divided by the number of population of each country (TOP 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+\n",
      "|Country/Region|              DEATHS|\n",
      "+--------------+--------------------+\n",
      "|      S. Korea|2.831664369584020...|\n",
      "|    Uzbekistan|8.448197037196562E-4|\n",
      "| Liechtenstein| 0.00233356931225255|\n",
      "|   Switzerland|0.004169056704159081|\n",
      "|        Norway|0.001745101945987...|\n",
      "|       Bahrain| 0.02513023907975126|\n",
      "|           USA|0.015193862960518527|\n",
      "|       Denmark|0.002468987081913...|\n",
      "|       Belgium|0.006137093728457364|\n",
      "|       Senegal|6.384103029353706E-4|\n",
      "+--------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avg = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "    `Country/Region`,\n",
    "\tAVG(CAST(TotalCases AS float)/Population) AS DEATHS\n",
    "    FROM worldometer_data\n",
    "    GROUP BY `Country/Region`\n",
    "    ORDER BY RAND()\n",
    "    LIMIT 10;\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "avg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
